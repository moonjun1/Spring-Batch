# 08. ë°°ì¹˜ í•™ìŠµ 4ë‹¨ê³„: ì„±ëŠ¥ ìµœì í™” ë° ë©€í‹°ìŠ¤ë ˆë“œ ì²˜ë¦¬

## ğŸ¯ í•™ìŠµ ëª©í‘œ
- ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ë¥¼ ìœ„í•œ ì„±ëŠ¥ ìµœì í™” ê¸°ë²• ìŠµë“
- ë©€í‹°ìŠ¤ë ˆë“œì™€ ë³‘ë ¬ ì²˜ë¦¬ êµ¬í˜„
- íŒŒí‹°ì…”ë‹(Partitioning)ì„ í†µí•œ ë¶„ì‚° ì²˜ë¦¬
- ë©”ëª¨ë¦¬ ìµœì í™” ë° ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

## ğŸ“Š ì„±ëŠ¥ ìµœì í™” ì „ëµ ê°œìš”

### ì„±ëŠ¥ ë³‘ëª© ì§€ì  ë¶„ì„
```
[ë°ì´í„° ì½ê¸°] â†’ [ë°ì´í„° ì²˜ë¦¬] â†’ [ë°ì´í„° ì“°ê¸°]
     â†“              â†“              â†“
  I/O ë³‘ëª©        CPU ë³‘ëª©      I/O + íŠ¸ëœì­ì…˜ ë³‘ëª©
     â†“              â†“              â†“
  - í˜ì´ì§•        - ë³µì¡í•œ        - ë°°ì¹˜ ì‚½ì…
  - ì¸ë±ìŠ¤         ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§    - íŠ¸ëœì­ì…˜ í¬ê¸°
  - ìºì‹±          - ë³€í™˜ ì‘ì—…      - ì»¤ë„¥ì…˜ í’€
```

### ìµœì í™” ê¸°ë²•ë“¤
1. **ì²­í¬ í¬ê¸° ìµœì í™”**: ë©”ëª¨ë¦¬ vs íŠ¸ëœì­ì…˜ ì˜¤ë²„í—¤ë“œ ê· í˜•
2. **ë©€í‹°ìŠ¤ë ˆë“œ ì²˜ë¦¬**: CPU í™œìš©ë„ ê·¹ëŒ€í™”
3. **íŒŒí‹°ì…”ë‹**: ëŒ€ìš©ëŸ‰ ë°ì´í„° ë¶„í•  ì²˜ë¦¬
4. **ë¹„ë™ê¸° ì²˜ë¦¬**: ItemProcessorì—ì„œ ì™¸ë¶€ API í˜¸ì¶œ ìµœì í™”
5. **ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™”**: ì¸ë±ìŠ¤, ë°°ì¹˜ ì‚½ì…, ì»¤ë„¥ì…˜ í’€

## ğŸš€ ë©€í‹°ìŠ¤ë ˆë“œ Step êµ¬í˜„

### 1. ê¸°ë³¸ ë©€í‹°ìŠ¤ë ˆë“œ ì„¤ì •
```java
package com.example.batchtutorial.config;

import lombok.extern.slf4j.Slf4j;
import org.springframework.batch.core.Job;
import org.springframework.batch.core.Step;
import org.springframework.batch.core.job.builder.JobBuilder;
import org.springframework.batch.core.repository.JobRepository;
import org.springframework.batch.core.step.builder.StepBuilder;
import org.springframework.batch.item.ItemProcessor;
import org.springframework.batch.item.ItemReader;
import org.springframework.batch.item.ItemWriter;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.core.task.TaskExecutor;
import org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor;
import org.springframework.transaction.PlatformTransactionManager;

/**
 * ë©€í‹°ìŠ¤ë ˆë“œ ë°°ì¹˜ ì²˜ë¦¬ ì„¤ì •
 */
@Slf4j
@Configuration
public class MultiThreadBatchConfig {
    
    @Autowired
    private JobRepository jobRepository;
    
    @Autowired
    private PlatformTransactionManager transactionManager;
    
    /**
     * ë©€í‹°ìŠ¤ë ˆë“œ ë°°ì¹˜ìš© TaskExecutor
     */
    @Bean
    public TaskExecutor batchTaskExecutor() {
        ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor();
        executor.setCorePoolSize(4);        // ê¸°ë³¸ ìŠ¤ë ˆë“œ ìˆ˜
        executor.setMaxPoolSize(8);         // ìµœëŒ€ ìŠ¤ë ˆë“œ ìˆ˜
        executor.setQueueCapacity(100);     // í ìš©ëŸ‰
        executor.setThreadNamePrefix("batch-thread-");
        executor.setKeepAliveSeconds(60);   // ìœ íœ´ ìŠ¤ë ˆë“œ ìƒì¡´ ì‹œê°„
        executor.setWaitForTasksToCompleteOnShutdown(true);
        executor.setAwaitTerminationSeconds(60);
        executor.initialize();
        return executor;
    }
    
    /**
     * CPU ì§‘ì•½ì  ì‘ì—…ìš© TaskExecutor
     */
    @Bean
    public TaskExecutor cpuIntensiveTaskExecutor() {
        ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor();
        // CPU ì½”ì–´ ìˆ˜ë§Œí¼ ìŠ¤ë ˆë“œ ì„¤ì • (CPU ì§‘ì•½ì  ì‘ì—…)
        int cpuCores = Runtime.getRuntime().availableProcessors();
        executor.setCorePoolSize(cpuCores);
        executor.setMaxPoolSize(cpuCores);
        executor.setQueueCapacity(50);
        executor.setThreadNamePrefix("cpu-batch-");
        executor.initialize();
        return executor;
    }
    
    /**
     * I/O ì§‘ì•½ì  ì‘ì—…ìš© TaskExecutor
     */
    @Bean
    public TaskExecutor ioIntensiveTaskExecutor() {
        ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor();
        // I/O ëŒ€ê¸° ì‹œê°„ì„ ê³ ë ¤í•˜ì—¬ ë” ë§ì€ ìŠ¤ë ˆë“œ í• ë‹¹
        executor.setCorePoolSize(10);
        executor.setMaxPoolSize(20);
        executor.setQueueCapacity(200);
        executor.setThreadNamePrefix("io-batch-");
        executor.initialize();
        return executor;
    }
    
    /**
     * ë©€í‹°ìŠ¤ë ˆë“œ Step ì„¤ì •
     */
    @Bean
    public Step multiThreadStep(ItemReader<Object> reader,
                               ItemProcessor<Object, Object> processor,
                               ItemWriter<Object> writer) {
        return new StepBuilder("multiThreadStep", jobRepository)
                .<Object, Object>chunk(100, transactionManager)  // í° ì²­í¬ í¬ê¸°
                .reader(reader)
                .processor(processor)
                .writer(writer)
                .taskExecutor(batchTaskExecutor())       // TaskExecutor ì„¤ì •
                .throttleLimit(4)                        // ë™ì‹œ ì‹¤í–‰ ìŠ¤ë ˆë“œ ì œí•œ
                .build();
    }
}
```

### 2. ìŠ¤ë ˆë“œ ì„¸ì´í”„ ItemReader êµ¬í˜„
```java
package com.example.batchtutorial.batch.reader;

import lombok.extern.slf4j.Slf4j;
import org.springframework.batch.item.database.JdbcPagingItemReader;
import org.springframework.batch.item.database.PagingQueryProvider;
import org.springframework.batch.item.database.builder.JdbcPagingItemReaderBuilder;
import org.springframework.batch.item.database.support.SqlPagingQueryProviderFactoryBean;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.jdbc.core.BeanPropertyRowMapper;

import javax.sql.DataSource;

/**
 * ë©€í‹°ìŠ¤ë ˆë“œ í™˜ê²½ì—ì„œ ì•ˆì „í•œ ItemReader ì„¤ì •
 */
@Slf4j
@Configuration
public class ThreadSafeReaderConfig {
    
    @Autowired
    private DataSource dataSource;
    
    /**
     * ìŠ¤ë ˆë“œ ì„¸ì´í”„í•œ JDBC Paging ItemReader
     * (JdbcPagingItemReaderëŠ” ê¸°ë³¸ì ìœ¼ë¡œ thread-safe)
     */
    @Bean
    public JdbcPagingItemReader<LargeDataDto> threadSafeReader() throws Exception {
        
        return new JdbcPagingItemReaderBuilder<LargeDataDto>()
                .name("threadSafeReader")
                .dataSource(dataSource)
                .queryProvider(createQueryProvider())
                .pageSize(1000)                    // í˜ì´ì§€ í¬ê¸° ì¦ê°€
                .rowMapper(new BeanPropertyRowMapper<>(LargeDataDto.class))
                .saveState(false)                  // ë©€í‹°ìŠ¤ë ˆë“œì—ì„œëŠ” ìƒíƒœ ì €ì¥ ë¹„í™œì„±í™”
                .build();
    }
    
    /**
     * í˜ì´ì§• ì¿¼ë¦¬ ì œê³µì ìƒì„±
     */
    private PagingQueryProvider createQueryProvider() throws Exception {
        SqlPagingQueryProviderFactoryBean factory = new SqlPagingQueryProviderFactoryBean();
        factory.setDataSource(dataSource);
        factory.setSelectClause("SELECT id, name, amount, created_at");
        factory.setFromClause("FROM large_data_table");
        factory.setWhereClause("WHERE status = 'ACTIVE'");
        factory.setSortKey("id");                  // ì •ë ¬ í‚¤ í•„ìˆ˜
        return factory.getObject();
    }
}

/**
 * ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ìš© DTO
 */
@Data
@NoArgsConstructor
@AllArgsConstructor
class LargeDataDto {
    private Long id;
    private String name;
    private BigDecimal amount;
    private LocalDateTime createdAt;
}
```

### 3. ë¹„ë™ê¸° ItemProcessor êµ¬í˜„
```java
package com.example.batchtutorial.batch.processor;

import lombok.extern.slf4j.Slf4j;
import org.springframework.batch.item.ItemProcessor;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.web.reactive.function.client.WebClient;
import reactor.core.publisher.Mono;

import java.time.Duration;
import java.util.concurrent.CompletableFuture;

/**
 * ë¹„ë™ê¸° ì²˜ë¦¬ë¥¼ í¬í•¨í•˜ëŠ” ItemProcessor
 */
@Slf4j
@Configuration
public class AsyncProcessorConfig {
    
    @Autowired
    private WebClient webClient;
    
    /**
     * ì™¸ë¶€ API í˜¸ì¶œì´ í¬í•¨ëœ ë¹„ë™ê¸° Processor
     */
    @Bean
    public ItemProcessor<LargeDataDto, ProcessedDataDto> asyncProcessor() {
        return new ItemProcessor<LargeDataDto, ProcessedDataDto>() {
            @Override
            public ProcessedDataDto process(LargeDataDto item) throws Exception {
                
                long startTime = System.currentTimeMillis();
                String threadName = Thread.currentThread().getName();
                
                log.debug("Processing item {} on thread {}", item.getId(), threadName);
                
                try {
                    // 1. ë¡œì»¬ ë°ì´í„° ë³€í™˜ (CPU ì‘ì—…)
                    ProcessedDataDto result = transformData(item);
                    
                    // 2. ì™¸ë¶€ API í˜¸ì¶œ (I/O ì‘ì—…)
                    String enrichedData = callExternalApiAsync(item.getId()).get();
                    result.setEnrichedData(enrichedData);
                    
                    // 3. ë³µì¡í•œ ê³„ì‚° ìˆ˜í–‰ (CPU ì‘ì—…)
                    BigDecimal calculatedValue = performComplexCalculation(item.getAmount());
                    result.setCalculatedValue(calculatedValue);
                    
                    long processingTime = System.currentTimeMillis() - startTime;
                    result.setProcessingTime(processingTime);
                    
                    log.debug("Completed processing item {} in {}ms on thread {}", 
                            item.getId(), processingTime, threadName);
                    
                    return result;
                    
                } catch (Exception e) {
                    log.error("Failed to process item {} on thread {}: {}", 
                            item.getId(), threadName, e.getMessage());
                    throw e;
                }
            }
        };
    }
    
    /**
     * ë°ì´í„° ë³€í™˜ ë¡œì§
     */
    private ProcessedDataDto transformData(LargeDataDto input) {
        ProcessedDataDto result = new ProcessedDataDto();
        result.setId(input.getId());
        result.setProcessedName(input.getName().toUpperCase());
        result.setOriginalAmount(input.getAmount());
        result.setProcessedAt(LocalDateTime.now());
        return result;
    }
    
    /**
     * ì™¸ë¶€ API ë¹„ë™ê¸° í˜¸ì¶œ
     */
    private CompletableFuture<String> callExternalApiAsync(Long id) {
        return webClient.get()
                .uri("/api/enrich/{id}", id)
                .retrieve()
                .bodyToMono(String.class)
                .timeout(Duration.ofSeconds(5))
                .toFuture();
    }
    
    /**
     * ë³µì¡í•œ ê³„ì‚° ìˆ˜í–‰ (CPU ì§‘ì•½ì )
     */
    private BigDecimal performComplexCalculation(BigDecimal input) {
        // ë³µì¡í•œ ìˆ˜í•™ì  ê³„ì‚° ì‹œë®¬ë ˆì´ì…˜
        BigDecimal result = input;
        for (int i = 0; i < 1000; i++) {
            result = result.multiply(new BigDecimal("1.001"));
        }
        return result.setScale(2, RoundingMode.HALF_UP);
    }
}

/**
 * ì²˜ë¦¬ëœ ë°ì´í„° DTO
 */
@Data
@NoArgsConstructor
@AllArgsConstructor
class ProcessedDataDto {
    private Long id;
    private String processedName;
    private BigDecimal originalAmount;
    private BigDecimal calculatedValue;
    private String enrichedData;
    private LocalDateTime processedAt;
    private Long processingTime;
}
```

## ğŸ”€ íŒŒí‹°ì…”ë‹(Partitioning) êµ¬í˜„

### 1. íŒŒí‹°ì…”ë‹ ê¸°ë³¸ ì„¤ì •
```java
package com.example.batchtutorial.config;

import lombok.extern.slf4j.Slf4j;
import org.springframework.batch.core.Job;
import org.springframework.batch.core.Step;
import org.springframework.batch.core.configuration.annotation.StepScope;
import org.springframework.batch.core.job.builder.JobBuilder;
import org.springframework.batch.core.partition.PartitionHandler;
import org.springframework.batch.core.partition.support.Partitioner;
import org.springframework.batch.core.partition.support.TaskExecutorPartitionHandler;
import org.springframework.batch.core.repository.JobRepository;
import org.springframework.batch.core.step.builder.StepBuilder;
import org.springframework.batch.item.ItemProcessor;
import org.springframework.batch.item.ItemReader;
import org.springframework.batch.item.ItemWriter;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.core.task.TaskExecutor;
import org.springframework.transaction.PlatformTransactionManager;

/**
 * íŒŒí‹°ì…”ë‹ì„ í†µí•œ ë¶„ì‚° ì²˜ë¦¬ ì„¤ì •
 */
@Slf4j
@Configuration
public class PartitioningBatchConfig {
    
    @Autowired
    private JobRepository jobRepository;
    
    @Autowired
    private PlatformTransactionManager transactionManager;
    
    /**
     * íŒŒí‹°ì…”ë‹ Job ì„¤ì •
     */
    @Bean
    public Job partitioningJob(Step managerStep) {
        return new JobBuilder("partitioningJob", jobRepository)
                .start(managerStep)
                .build();
    }
    
    /**
     * ë§¤ë‹ˆì € Step (íŒŒí‹°ì…˜ì„ ê´€ë¦¬í•˜ëŠ” Step)
     */
    @Bean
    public Step managerStep(Step workerStep, Partitioner customPartitioner, TaskExecutor taskExecutor) {
        return new StepBuilder("managerStep", jobRepository)
                .partitioner("workerStep", customPartitioner)
                .step(workerStep)
                .partitionHandler(partitionHandler(workerStep, taskExecutor))
                .build();
    }
    
    /**
     * íŒŒí‹°ì…˜ í•¸ë“¤ëŸ¬ ì„¤ì •
     */
    @Bean
    public PartitionHandler partitionHandler(Step workerStep, TaskExecutor taskExecutor) {
        TaskExecutorPartitionHandler handler = new TaskExecutorPartitionHandler();
        handler.setTaskExecutor(taskExecutor);
        handler.setStep(workerStep);
        handler.setGridSize(8);                    // íŒŒí‹°ì…˜ ê°œìˆ˜
        return handler;
    }
    
    /**
     * ì›Œì»¤ Step (ì‹¤ì œ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” Step)
     */
    @Bean
    public Step workerStep(ItemReader<LargeDataDto> partitionReader,
                          ItemProcessor<LargeDataDto, ProcessedDataDto> processor,
                          ItemWriter<ProcessedDataDto> writer) {
        return new StepBuilder("workerStep", jobRepository)
                .<LargeDataDto, ProcessedDataDto>chunk(500, transactionManager)
                .reader(partitionReader)
                .processor(processor)
                .writer(writer)
                .build();
    }
    
    /**
     * ì»¤ìŠ¤í…€ íŒŒí‹°ì…”ë„ˆ - ë°ì´í„° ë²”ìœ„ì— ë”°ë¼ ë¶„í• 
     */
    @Bean
    public Partitioner customPartitioner() {
        return new CustomRangePartitioner();
    }
}
```

### 2. ì»¤ìŠ¤í…€ íŒŒí‹°ì…”ë„ˆ êµ¬í˜„
```java
package com.example.batchtutorial.partition;

import lombok.extern.slf4j.Slf4j;
import org.springframework.batch.core.partition.support.Partitioner;
import org.springframework.batch.item.ExecutionContext;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.jdbc.core.JdbcTemplate;
import org.springframework.stereotype.Component;

import java.util.HashMap;
import java.util.Map;

/**
 * ID ë²”ìœ„ ê¸°ë°˜ íŒŒí‹°ì…”ë„ˆ
 */
@Slf4j
@Component
public class CustomRangePartitioner implements Partitioner {
    
    @Autowired
    private JdbcTemplate jdbcTemplate;
    
    @Override
    public Map<String, ExecutionContext> partition(int gridSize) {
        
        log.info("Creating {} partitions for data processing", gridSize);
        
        // 1. ì „ì²´ ë°ì´í„° ë²”ìœ„ ì¡°íšŒ
        Long minId = jdbcTemplate.queryForObject("SELECT MIN(id) FROM large_data_table WHERE status = 'ACTIVE'", Long.class);
        Long maxId = jdbcTemplate.queryForObject("SELECT MAX(id) FROM large_data_table WHERE status = 'ACTIVE'", Long.class);
        Long totalRecords = jdbcTemplate.queryForObject("SELECT COUNT(*) FROM large_data_table WHERE status = 'ACTIVE'", Long.class);
        
        if (minId == null || maxId == null || totalRecords == 0) {
            log.warn("No data found for partitioning");
            return new HashMap<>();
        }
        
        log.info("Total records: {}, ID range: {} - {}", totalRecords, minId, maxId);
        
        // 2. íŒŒí‹°ì…˜ë³„ ë²”ìœ„ ê³„ì‚°
        long rangeSize = (maxId - minId + 1) / gridSize;
        Map<String, ExecutionContext> partitions = new HashMap<>();
        
        for (int i = 0; i < gridSize; i++) {
            ExecutionContext context = new ExecutionContext();
            
            long startId = minId + (i * rangeSize);
            long endId = (i == gridSize - 1) ? maxId : startId + rangeSize - 1;
            
            context.putLong("startId", startId);
            context.putLong("endId", endId);
            
            // íŒŒí‹°ì…˜ë³„ ì˜ˆìƒ ë ˆì½”ë“œ ìˆ˜ ê³„ì‚°
            Long partitionRecordCount = jdbcTemplate.queryForObject(
                "SELECT COUNT(*) FROM large_data_table WHERE id BETWEEN ? AND ? AND status = 'ACTIVE'",
                Long.class, startId, endId
            );
            
            context.putLong("expectedRecords", partitionRecordCount);
            
            partitions.put("partition" + i, context);
            
            log.info("Partition {}: ID range {} - {}, expected records: {}", 
                    i, startId, endId, partitionRecordCount);
        }
        
        return partitions;
    }
}
```

### 3. íŒŒí‹°ì…˜ìš© StepScope ItemReader
```java
package com.example.batchtutorial.batch.reader;

import lombok.extern.slf4j.Slf4j;
import org.springframework.batch.core.configuration.annotation.StepScope;
import org.springframework.batch.item.database.JdbcPagingItemReader;
import org.springframework.batch.item.database.builder.JdbcPagingItemReaderBuilder;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.jdbc.core.BeanPropertyRowMapper;

import javax.sql.DataSource;
import java.util.HashMap;
import java.util.Map;

/**
 * íŒŒí‹°ì…˜ë³„ ë°ì´í„°ë¥¼ ì½ëŠ” StepScope ItemReader
 */
@Slf4j
@Configuration
public class PartitionReaderConfig {
    
    @Autowired
    private DataSource dataSource;
    
    /**
     * íŒŒí‹°ì…˜ ë²”ìœ„ì— ë”°ë¼ ë°ì´í„°ë¥¼ ì½ëŠ” Reader
     */
    @Bean
    @StepScope
    public JdbcPagingItemReader<LargeDataDto> partitionReader(
            @Value("#{stepExecutionContext['startId']}") Long startId,
            @Value("#{stepExecutionContext['endId']}") Long endId,
            @Value("#{stepExecutionContext['expectedRecords']}") Long expectedRecords) {
        
        String threadName = Thread.currentThread().getName();
        log.info("Creating partition reader for range {} - {} (expected: {} records) on thread {}", 
                startId, endId, expectedRecords, threadName);
        
        Map<String, Object> parameters = new HashMap<>();
        parameters.put("startId", startId);
        parameters.put("endId", endId);
        
        return new JdbcPagingItemReaderBuilder<LargeDataDto>()
                .name("partitionReader")
                .dataSource(dataSource)
                .queryString("SELECT id, name, amount, created_at FROM large_data_table WHERE id BETWEEN :startId AND :endId AND status = 'ACTIVE'")
                .parameterValues(parameters)
                .pageSize(100)
                .rowMapper(new BeanPropertyRowMapper<>(LargeDataDto.class))
                .saveState(false)  // íŒŒí‹°ì…”ë‹ì—ì„œëŠ” ìƒíƒœ ì €ì¥ ë¹„í™œì„±í™”
                .build();
    }
}
```

## ğŸ“ˆ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ì¸¡ì •

### 1. ë°°ì¹˜ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë¦¬ìŠ¤ë„ˆ
```java
package com.example.batchtutorial.listener;

import lombok.extern.slf4j.Slf4j;
import org.springframework.batch.core.JobExecution;
import org.springframework.batch.core.JobExecutionListener;
import org.springframework.batch.core.StepExecution;
import org.springframework.stereotype.Component;

import java.time.Duration;
import java.time.LocalDateTime;
import java.time.ZoneId;

/**
 * ë°°ì¹˜ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë¦¬ìŠ¤ë„ˆ
 */
@Slf4j
@Component
public class PerformanceMonitoringListener implements JobExecutionListener {
    
    @Override
    public void beforeJob(JobExecution jobExecution) {
        log.info("ğŸš€ Job started: {} at {}", 
                jobExecution.getJobInstance().getJobName(),
                LocalDateTime.ofInstant(jobExecution.getStartTime().toInstant(), ZoneId.systemDefault()));
        
        // ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§
        Runtime runtime = Runtime.getRuntime();
        long maxMemory = runtime.maxMemory();
        long freeMemory = runtime.freeMemory();
        long totalMemory = runtime.totalMemory();
        
        log.info("ğŸ’¾ Memory status - Max: {}MB, Free: {}MB, Total: {}MB", 
                maxMemory / (1024 * 1024), 
                freeMemory / (1024 * 1024), 
                totalMemory / (1024 * 1024));
        
        log.info("ğŸ–¥ï¸ Available processors: {}", runtime.availableProcessors());
    }
    
    @Override
    public void afterJob(JobExecution jobExecution) {
        Duration duration = Duration.between(
                jobExecution.getStartTime().toInstant(),
                jobExecution.getEndTime().toInstant()
        );
        
        log.info("âœ… Job completed: {} in {} seconds", 
                jobExecution.getJobInstance().getJobName(),
                duration.getSeconds());
        
        // Stepë³„ ì„±ëŠ¥ í†µê³„
        logStepStatistics(jobExecution);
        
        // ì „ì²´ ì„±ëŠ¥ í†µê³„
        logOverallPerformance(jobExecution, duration);
        
        // ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì¢… í™•ì¸
        logFinalMemoryUsage();
    }
    
    private void logStepStatistics(JobExecution jobExecution) {
        log.info("ğŸ“Š Step Performance Statistics:");
        
        for (StepExecution stepExecution : jobExecution.getStepExecutions()) {
            Duration stepDuration = Duration.between(
                    stepExecution.getStartTime().toInstant(),
                    stepExecution.getEndTime().toInstant()
            );
            
            long readCount = stepExecution.getReadCount();
            long writeCount = stepExecution.getWriteCount();
            long skipCount = stepExecution.getSkipCount();
            
            double readThroughput = readCount / (double) stepDuration.getSeconds();
            double writeThroughput = writeCount / (double) stepDuration.getSeconds();
            
            log.info("  ğŸ“‹ Step: {}", stepExecution.getStepName());
            log.info("    â±ï¸ Duration: {} seconds", stepDuration.getSeconds());
            log.info("    ğŸ“– Read: {} items ({:.2f} items/sec)", readCount, readThroughput);
            log.info("    âœï¸ Write: {} items ({:.2f} items/sec)", writeCount, writeThroughput);
            log.info("    âš ï¸ Skip: {} items", skipCount);
            
            if (stepExecution.getCommitCount() > 0) {
                double avgChunkSize = (double) writeCount / stepExecution.getCommitCount();
                log.info("    ğŸ”„ Commits: {} (avg chunk size: {:.1f})", 
                        stepExecution.getCommitCount(), avgChunkSize);
            }
        }
    }
    
    private void logOverallPerformance(JobExecution jobExecution, Duration duration) {
        long totalReadCount = jobExecution.getStepExecutions().stream()
                .mapToLong(StepExecution::getReadCount)
                .sum();
        
        long totalWriteCount = jobExecution.getStepExecutions().stream()
                .mapToLong(StepExecution::getWriteCount)
                .sum();
        
        double overallThroughput = totalWriteCount / (double) duration.getSeconds();
        
        log.info("ğŸ¯ Overall Performance:");
        log.info("  ğŸ“– Total Read: {} items", totalReadCount);
        log.info("  âœï¸ Total Write: {} items", totalWriteCount);
        log.info("  âš¡ Overall Throughput: {:.2f} items/sec", overallThroughput);
        log.info("  ğŸ“Š Success Rate: {:.2f}%", 
                (totalWriteCount / (double) totalReadCount) * 100);
    }
    
    private void logFinalMemoryUsage() {
        Runtime runtime = Runtime.getRuntime();
        runtime.gc(); // ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ìˆ˜í–‰
        
        long usedMemory = runtime.totalMemory() - runtime.freeMemory();
        long maxMemory = runtime.maxMemory();
        
        log.info("ğŸ’¾ Final Memory Usage: {}MB / {}MB ({:.1f}%)", 
                usedMemory / (1024 * 1024),
                maxMemory / (1024 * 1024),
                (usedMemory / (double) maxMemory) * 100);
    }
}
```

### 2. ì„±ëŠ¥ ìµœì í™”ëœ ItemWriter
```java
package com.example.batchtutorial.batch.writer;

import lombok.extern.slf4j.Slf4j;
import org.springframework.batch.item.Chunk;
import org.springframework.batch.item.database.JdbcBatchItemWriter;
import org.springframework.batch.item.database.builder.JdbcBatchItemWriterBuilder;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.jdbc.core.namedparam.MapSqlParameterSource;

import javax.sql.DataSource;

/**
 * ê³ ì„±ëŠ¥ ë°°ì¹˜ ì‚½ì…ì„ ìœ„í•œ ItemWriter
 */
@Slf4j
@Configuration  
public class HighPerformanceWriterConfig {
    
    @Autowired
    private DataSource dataSource;
    
    /**
     * JDBC ë°°ì¹˜ ì‚½ì… Writer
     */
    @Bean
    public JdbcBatchItemWriter<ProcessedDataDto> highPerformanceWriter() {
        return new JdbcBatchItemWriterBuilder<ProcessedDataDto>()
                .dataSource(dataSource)
                .sql("INSERT INTO processed_data (id, processed_name, original_amount, calculated_value, enriched_data, processed_at, processing_time) " +
                     "VALUES (:id, :processedName, :originalAmount, :calculatedValue, :enrichedData, :processedAt, :processingTime)")
                .beanMapped()
                .assertUpdates(false)  // ì—…ë°ì´íŠ¸ í™•ì¸ ë¹„í™œì„±í™” (ì„±ëŠ¥ í–¥ìƒ)
                .build();
    }
    
    /**
     * ì‚¬ìš©ì ì •ì˜ ë°°ì¹˜ Writer (ë” ì„¸ë°€í•œ ì œì–´)
     */
    @Bean
    public CustomBatchWriter customBatchWriter() {
        return new CustomBatchWriter(dataSource);
    }
}

/**
 * ì»¤ìŠ¤í…€ ë°°ì¹˜ Writer êµ¬í˜„
 */
@Slf4j
class CustomBatchWriter implements ItemWriter<ProcessedDataDto> {
    
    private final DataSource dataSource;
    private final JdbcTemplate jdbcTemplate;
    
    public CustomBatchWriter(DataSource dataSource) {
        this.dataSource = dataSource;
        this.jdbcTemplate = new JdbcTemplate(dataSource);
    }
    
    @Override
    public void write(Chunk<? extends ProcessedDataDto> chunk) throws Exception {
        List<? extends ProcessedDataDto> items = chunk.getItems();
        
        if (items.isEmpty()) {
            return;
        }
        
        long startTime = System.currentTimeMillis();
        String threadName = Thread.currentThread().getName();
        
        log.debug("Writing {} items on thread {}", items.size(), threadName);
        
        try {
            // ë°°ì¹˜ ì‚½ì… ì¤€ë¹„
            String sql = "INSERT INTO processed_data " +
                        "(id, processed_name, original_amount, calculated_value, enriched_data, processed_at, processing_time) " +
                        "VALUES (?, ?, ?, ?, ?, ?, ?)";
            
            List<Object[]> batchArgs = items.stream()
                    .map(this::createBatchArgs)
                    .collect(Collectors.toList());
            
            // ë°°ì¹˜ ì‹¤í–‰
            int[] results = jdbcTemplate.batchUpdate(sql, batchArgs);
            
            long executionTime = System.currentTimeMillis() - startTime;
            double throughput = items.size() / (executionTime / 1000.0);
            
            log.debug("Batch write completed - {} items in {}ms ({:.2f} items/sec) on thread {}", 
                    items.size(), executionTime, throughput, threadName);
            
            // ì‹¤íŒ¨í•œ ì‚½ì… í™•ì¸
            long failedInserts = Arrays.stream(results).filter(result -> result == 0).count();
            if (failedInserts > 0) {
                log.warn("âš ï¸ {} insertions failed out of {} on thread {}", 
                        failedInserts, items.size(), threadName);
            }
            
        } catch (Exception e) {
            log.error("âŒ Batch write failed for {} items on thread {}: {}", 
                    items.size(), threadName, e.getMessage(), e);
            throw e;
        }
    }
    
    private Object[] createBatchArgs(ProcessedDataDto item) {
        return new Object[]{
            item.getId(),
            item.getProcessedName(),
            item.getOriginalAmount(),
            item.getCalculatedValue(),
            item.getEnrichedData(),
            item.getProcessedAt(),
            item.getProcessingTime()
        };
    }
}
```

## ğŸ”§ ì„±ëŠ¥ ìµœì í™” ì„¤ì •

### 1. ë°ì´í„°ë² ì´ìŠ¤ ì»¤ë„¥ì…˜ í’€ ìµœì í™”
```properties
# application-performance.properties

# HikariCP ì„¤ì • (ê³ ì„±ëŠ¥ ì»¤ë„¥ì…˜ í’€)
spring.datasource.hikari.maximum-pool-size=20
spring.datasource.hikari.minimum-idle=10
spring.datasource.hikari.idle-timeout=300000
spring.datasource.hikari.max-lifetime=600000
spring.datasource.hikari.connection-timeout=30000
spring.datasource.hikari.leak-detection-threshold=60000

# JPA/Hibernate ì„±ëŠ¥ ìµœì í™”
spring.jpa.hibernate.jdbc.batch_size=25
spring.jpa.hibernate.order_inserts=true
spring.jpa.hibernate.order_updates=true
spring.jpa.hibernate.jdbc.batch_versioned_data=true

# Spring Batch ì„±ëŠ¥ ì„¤ì •
spring.batch.jdbc.isolation-level-for-create=READ_COMMITTED
spring.batch.jdbc.table-prefix=BATCH_

# ë¡œê¹… ì„¤ì • (ì„±ëŠ¥ ëª¨ë“œ)
logging.level.org.springframework.batch=INFO
logging.level.org.hibernate.SQL=WARN
logging.level.org.hibernate.type.descriptor.sql.BasicBinder=WARN
```

### 2. JVM ì„±ëŠ¥ íŠœë‹ ì˜µì…˜
```bash
# ë°°ì¹˜ ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹¤í–‰ ì‹œ ê¶Œì¥ JVM ì˜µì…˜
java -Xms2g -Xmx4g \
     -XX:+UseG1GC \
     -XX:MaxGCPauseMillis=200 \
     -XX:+UnlockExperimentalVMOptions \
     -XX:+UseStringDeduplication \
     -XX:+PrintGCDetails \
     -XX:+PrintGCTimeStamps \
     -Xloggc:gc.log \
     -jar batch-application.jar
```

## ğŸ“Š ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ë° ë²¤ì¹˜ë§ˆí¬

### ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤
```java
package com.example.batchtutorial.test;

import lombok.extern.slf4j.Slf4j;
import org.junit.jupiter.api.Test;
import org.springframework.batch.core.Job;
import org.springframework.batch.core.JobParameters;
import org.springframework.batch.core.JobParametersBuilder;
import org.springframework.batch.core.launch.JobLauncher;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.boot.test.context.SpringBootTest;
import org.springframework.test.context.ActiveProfiles;

import java.time.Duration;
import java.time.Instant;

/**
 * ë°°ì¹˜ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
 */
@Slf4j
@SpringBootTest
@ActiveProfiles("performance")
class BatchPerformanceTest {
    
    @Autowired
    private JobLauncher jobLauncher;
    
    @Autowired
    private Job singleThreadJob;
    
    @Autowired
    private Job multiThreadJob;
    
    @Autowired
    private Job partitioningJob;
    
    @Test
    void compareBatchPerformance() throws Exception {
        
        // 1. ë‹¨ì¼ ìŠ¤ë ˆë“œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
        Instant start = Instant.now();
        jobLauncher.run(singleThreadJob, createJobParameters("single"));
        Duration singleThreadTime = Duration.between(start, Instant.now());
        log.info("Single thread execution time: {} seconds", singleThreadTime.getSeconds());
        
        // 2. ë©€í‹° ìŠ¤ë ˆë“œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
        start = Instant.now();
        jobLauncher.run(multiThreadJob, createJobParameters("multi"));
        Duration multiThreadTime = Duration.between(start, Instant.now());
        log.info("Multi thread execution time: {} seconds", multiThreadTime.getSeconds());
        
        // 3. íŒŒí‹°ì…”ë‹ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
        start = Instant.now();
        jobLauncher.run(partitioningJob, createJobParameters("partition"));
        Duration partitioningTime = Duration.between(start, Instant.now());
        log.info("Partitioning execution time: {} seconds", partitioningTime.getSeconds());
        
        // 4. ì„±ëŠ¥ ë¹„êµ ê²°ê³¼
        log.info("Performance Comparison:");
        log.info("  Single Thread: {} seconds (baseline)", singleThreadTime.getSeconds());
        log.info("  Multi Thread: {} seconds ({}x faster)", 
                multiThreadTime.getSeconds(), 
                (double) singleThreadTime.getSeconds() / multiThreadTime.getSeconds());
        log.info("  Partitioning: {} seconds ({}x faster)", 
                partitioningTime.getSeconds(),
                (double) singleThreadTime.getSeconds() / partitioningTime.getSeconds());
    }
    
    private JobParameters createJobParameters(String suffix) {
        return new JobParametersBuilder()
                .addLong("timestamp", System.currentTimeMillis())
                .addString("testType", suffix)
                .toJobParameters();
    }
}
```

ì´ì œ ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ë¥¼ ìœ„í•œ ê³ ì„±ëŠ¥ Spring Batch ì‹œìŠ¤í…œì„ ì™„ì „íˆ êµ¬í˜„í–ˆìŠµë‹ˆë‹¤! ë‹¤ìŒ ë‹¨ê³„ì—ì„œëŠ” ì‹¤ë¬´ì—ì„œ í•„ìš”í•œ ì—ëŸ¬ ì²˜ë¦¬ì™€ ëª¨ë‹ˆí„°ë§ ê¸°ëŠ¥ì„ í•™ìŠµí•˜ê² ìŠµë‹ˆë‹¤.